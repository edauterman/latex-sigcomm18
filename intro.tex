\section{Introduction}

\indent Today's data center applications depend on replication to ensure data availability and transparently mask server failures. Unfortunately, consensus protocols like Paxos\cite{paxos,paxossimple} and Raft\cite{raft} that provide replication with strong consistency guarantees also impose a significant performance overhead. Network-Ordered Paxos\cite{nopaxos} (NOPaxos) dramatically reduces this performance overhead by splitting the replication responsibility between the network and protocol layers. The authors of NOPaxos present a new network primitive to do this: ordered unreliable multicast (OUM). 

OUM provides semantics strong enough to reduce overhead at the protocol layer, but weak enough that OUM can be implemented at near-zero-cost within a datacenter. OUM is asynchronous and unreliable, and it guarantees that all replicas receive messages in the same order. All packets for a OUM group are routed through a single sequencer, which determines the global packet order. The authors describe three places in the network to implement the sequencer: in the switches themselves, in hardware middleboxes, and at end-hosts. OUM greatly simplifies the task of the protocol layer: instead of agreeing on the total ordering of client requests, now replicas only have to agree on which requests to execute and which to permanently ignore. 

\subsection{NOPaxos on a Cloud Platform}
    
Cloud platforms have become increasingly popular because they allow users to pay for resources used without significant up-front investment and they move the burden of maintaining a cluster from the user to the cloud provider. Unfortunately, NOPaxos as presented in the original paper \cite{nopaxos} cannot run on a cloud platform because it relies on multicast. Most mainstream cloud providers such as Amazon AWS and Google Cloud Platform do not support multicast because all instances run on the same internal network, and so multicast could overload the network and negatively impact performance. 

Given the advantages of cloud platforms, we explore adapting NOPaxos to a cloud platform and evaluate its performance in this environment with the necessary modifications. Specifically, we compare the throughput, latency, and scaling capabilities of NOPaxos to those of three other consensus protocols (paxos, paxos with batching, and fastpaxos) and an unreplicated system when running on a cloud platform. We contrast these results with those of the original NOPaxos paper \cite{nopaxos}, which conducted its evaluation on dedicated hardware.

We find that the modifications necessary to allow NOPaxos to run on a cloud platform create a bottleneck that prevent it from scaling as it does on dedicated hardware. Therefore, although NOPaxos on a cloud platform can achieve comparable latency under low load, it is unable match the throughput achieved on dedicated hardware with a greater number of replicas. However, we show that in some circumstances, NOPaxos in the cloud still performs better than other consensus protocols in the same environment.

The reminder of the paper proceeds as follows. In Section 2, we discuss related work regarding the reduction of replication overhead in consensus protocols and how this compares to NOPaxos, especially in the context of testing on cloud platforms. In Section 3, we outline the design and setup of our experiments with NOPaxos on a cloud platform, including the modifications that had to be made to the original code base. In Section 4, we contrast the results of these experiments with those run by the authors of \cite{nopaxos} on dedicated hardware, focusing in particular on our reproductions of Figures 5 and 8 from the original paper; we also identify and evaluate the bottleneck that we discovered during the course of these experiments. In Section 5, we discuss the limitations of our experiment and of NOPaxos on a cloud platform. In Section 6, we outline future work for the development and evaluation of consensus protocols (such as NOPaxos) on cloud platforms. Finally, in Section 6, we conclude with a summary of our results and lessons learned. 