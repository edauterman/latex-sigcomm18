\section{Related Work} \label{related}

Other consensus protocols have also tried to reduce the performance overhead of replication. \cite{pbft} shows how vanilla Paxos can be modified to batch requests, using a sliding-window where the system adaptively adjusts the batch size. This is shown to reduce latency at low load while still providing throughput benefits at high load \cite{pbft}.

Speculative Paxos\cite{specpaxos} and NetPaxos\cite{netpaxos} take a similar approach to NOPaxos by pushing functionality into the network (and thus also require modifications to run on a cloud platform).

Speculative Paxos uses a Mostly-Ordered Multicast primitive that provides a best-effort ordering property, but because it is not a guarantee, both superquorums and speculation are necessary. NOPaxos avoids protocol complexity by pushing more responsibility into the network layer. Conversely, NetPaxos moves Paxos logic into the switches (using P4), requiring switches to store the results of each consensus instance, which could become a large amount of state. While NetPaxos requires changes to the switch firmware, NOPaxos has much more realistic expectations of switches, requiring only the OUM primitive. For these reasons, we chose to modify and evaluate NOPaxos over SpecPaxos and NetPaxos. 

Fast Paxos\cite{fastpaxos}, Generalized Paxos\cite{generalizedpaxos}, and Egalitarian Paxos\cite{epaxos} (EPaxos) take a different approach, making no assumptions about the network (and thus do not require modifications to run on a cloud platform). 

Fast Paxos allows clients to send operations directly to replicas to reduce overhead, but if multiple clients concurrently send operations, competing to commit, extra messages are required to determine ordering (NOPaxos relies on the network to do this). Generalized Paxos and EPaxos both exploit commutativity to improve performance: Generalized Paxos is similar to Fast Paxos but solves the problem of contention with multiple cilents by exploiting commutativity, and EPaxos uses multiple leaders to propose and execute operations concurrently. For both Generalized Paxos and EPaxos, performance depends on the nature of the workload (i.e. the degree of commutativity), and so cannot achieve the consistency of performance across workloads in NOPaxos. However, these protocols make no assumptions about the network, and so are good choices outside datacenters. 

The authors of the original paper\cite{nopaxos} compare NOPaxos to both Speculative Paxos and Fast Paxos (in addition to vanilla Paxos and Paxos with Batching) on dedicated hardware. We likewise include a comparison of NOPaxos to Fast Paxos, vanilla Paxos, and Paxos with Batching on a cloud platform. However, due to time constraints and the need for modifications to the original protocol, we leave a comparison of NOPaxos to Speculative Paxos on a cloud platform to future work. 